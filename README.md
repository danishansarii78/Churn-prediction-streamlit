# Churn-prediction-streamlit
ğŸ¯ Project Overview
This project analyzes customer churn data to predict which customers are likely to leave a telecommunications service. Using machine learning, we can identify at-risk customers and help businesses take proactive retention measures.

ğŸ“Š Key Insights
Dataset: 7,032 customers with 31 features

Accuracy: 79% with Random Forest Classifier

Business Impact: Identify 48% of potential churners early

ğŸš€ Quick Start
Prerequisites
bash
pip install pandas numpy scikit-learn jupyter
Run the Project
bash
# Clone this repository
git clone https://github.com/danishansarii78/Churn-prediction-streamlit.git

# Navigate to project directory
cd Churn-prediction-streamlit

# Launch Jupyter Notebook
jupyter notebook
Open CUSTOMER_CHURN.ipynb and run all cells to see the magic! âœ¨

ğŸ“ˆ Model Performance
Confusion Matrix
text
[[926 107]  â† Correctly predicted non-churners: 926
 [193 181]] â† Correctly predicted churners: 181
Classification Report
Class	Precision	Recall	F1-Score	Support
No Churn (0)	83%	90%	86%	1,033
Churn (1)	63%	48%	55%	374
Overall Accuracy	79%			1,407
ğŸ› ï¸ Technical Implementation
Data Preprocessing
python
# Key steps in our pipeline:
1. âœ… Handle missing values in TotalCharges
2. âœ… Convert Churn to binary (Yesâ†’1, Noâ†’0)
3. âœ… One-Hot Encoding for categorical variables
4. âœ… Train-Test Split (80-20)
Model Architecture
Algorithm: Random Forest Classifier

Features: 30 engineered features after encoding

Target: Churn (Binary Classification)

Saved Artifacts
churn_model.pkl - Trained Random Forest model

churn_features.pkl - Feature columns for prediction

ğŸ’¡ Business Applications
ğŸ¯ Use Cases
Customer Retention: Identify high-risk customers for targeted offers

Resource Allocation: Focus retention efforts where they matter most

Product Insights: Understand which features drive customer satisfaction

ğŸ“Š Actionable Insights
The model can identify nearly half of potential churners

79% overall accuracy in predicting customer behavior

Can be integrated into CRM systems for real-time predictions

ğŸ—‚ï¸ Project Structure
text
customer-churn-prediction/
â”‚
â”œâ”€â”€ CUSTOMER_CHURN.ipynb          # Main analysis notebook
â”œâ”€â”€ churn_model.pkl              # Trained model (generated)
â”œâ”€â”€ churn_features.pkl           # Feature columns (generated)
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                   # This file
ğŸ® Interactive Demo
Want to test the model with your own data? Here's how:

python
import pickle
import pandas as pd

# Load the trained model
model = pickle.load(open('churn_model.pkl', 'rb'))
features = pickle.load(open('churn_features.pkl', 'rb'))

# Create sample input (replace with your data)
sample_data = {
    'tenure': 12,
    'MonthlyCharges': 70.5,
    'TotalCharges': 850.0,
    'Contract_Two year': 1,
    # ... add all other features
}

# Make prediction
prediction = model.predict([sample_data])
probability = model.predict_proba([sample_data])

print(f"Churn Prediction: {'Yes' if prediction[0] == 1 else 'No'}")
print(f"Confidence: {probability[0][1]:.2%}")
ğŸ“Š Results Interpretation
ğŸ¯ What the Metrics Mean
Precision (63%): When we predict churn, we're correct 63% of the time

Recall (48%): We catch 48% of all actual churn cases

F1-Score (55%): Balanced measure of precision and recall

ğŸ” Model Strengths
âœ… Excellent at identifying loyal customers (90% recall for class 0)

âœ… Good overall accuracy for business decisions

âœ… Handles imbalanced data reasonably well

ğŸš§ Areas for Improvement
âš ï¸ Could improve churn detection rate (currently 48%)

âš ï¸ Feature engineering could be enhanced

âš ï¸ Consider handling class imbalance with techniques like SMOTE
